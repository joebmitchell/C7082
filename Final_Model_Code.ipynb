{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chicken Disease Image Classification"
      ],
      "metadata": {
        "id": "BzXL1sbRUjuF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code chunks contain the code required to either retrain the final model from scratch or in section 2 download and prepare the trained model"
      ],
      "metadata": {
        "id": "oXlcJQqXgOF4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Creating a new model"
      ],
      "metadata": {
        "id": "sNszQ-tAK5vo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Setting seed for reproducability \n",
        "\n",
        "from numpy.random import seed\n",
        "seed(1)\n",
        "from tensorflow.random import set_seed\n",
        "set_seed(2)\n",
        "\n",
        "## Setting up required packages\n",
        "!pip install split-folders\n",
        "\n",
        "import os\n",
        "import os.path\n",
        "import shutil\n",
        "import splitfolders\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.text as txt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, callbacks\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "from google.colab import files\n",
        "from keras.layers.rnn import time_distributed\n",
        "import time\n",
        "import seaborn as sn\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.applications import EfficientNetB3"
      ],
      "metadata": {
        "id": "1T4MV-avh0Mq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mergefolders(root_src_dir, root_dst_dir): # create function to merge folders\n",
        "    \"\"\"\n",
        "    Merges two folders into one\n",
        "    root_src_dir: str - origin folder\n",
        "    root_dst_dir: str - destination folder\n",
        "    \n",
        "    \"\"\"\n",
        "    for src_dir, dirs, files in os.walk(root_src_dir):\n",
        "          dst_dir = src_dir.replace(root_src_dir, root_dst_dir, 1)\n",
        "          if not os.path.exists(dst_dir):\n",
        "              os.makedirs(dst_dir)\n",
        "          for file_ in files:\n",
        "              src_file = os.path.join(src_dir, file_)\n",
        "              dst_file = os.path.join(dst_dir, file_)\n",
        "              if os.path.exists(dst_file):\n",
        "                  os.remove(dst_file)\n",
        "              shutil.copy(src_file, dst_dir)\n"
      ],
      "metadata": {
        "id": "BRyq9zZFh6VL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(test = 0.1, train = 0.8, validate = 0.1):\n",
        "  \n",
        "  \"\"\"\n",
        "  Removes any previously downloaded data then downloads 'chicken-disease-1' data from kaggle.\n",
        "  Splits data into seperate folders based on disease, then splits data into \n",
        "  test, train and validate using a default of 80/10/10\n",
        "\n",
        "    test: float - proportion of data to be partitioned into test data\n",
        "    train: float - proportion of data to be partitioined into training data\n",
        "    validate: float - proportion of data to be partitioned into validation data\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        " # remove any previously downloaded data\n",
        "  !rm -rf '/content/Train'\n",
        "  !rm -rf '/content/train_data.csv'\n",
        "  !rm -rf '/content/output'\n",
        "  !rm -rf '/content/chicken-disease-1 .zip'\n",
        "\n",
        "  !kaggle datasets download -d 'allandclive/chicken-disease-1 ' -p /content #download data\n",
        "  !unzip -qq 'chicken-disease-1 .zip' # unzip data\n",
        "  \n",
        "\n",
        "  folder_path = '/content/Train' # Set path where images are\n",
        "\n",
        "# create list of images to go through below\n",
        "  images = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
        "\n",
        "  for image in images:\n",
        "    folder_name = image.split('.')[0] # Creates folder based on first part of name\n",
        "\n",
        "    new_path = os.path.join(folder_path, folder_name) # creates new path name\n",
        "    if not os.path.exists(new_path): # checks if path exixts\n",
        "      os.makedirs(new_path)  # if doesn't exists creates it\n",
        "\n",
        "    old_image_path = os.path.join(folder_path, image)\n",
        "    new_image_path = os.path.join(new_path, image)\n",
        "    shutil.move(old_image_path, new_image_path) # moves picture\n",
        "\n",
        "\n",
        "  # merge all folders with similar contents\n",
        "\n",
        "  mergefolders('/content/Train/pcrcocci', '/content/Train/cocci')\n",
        "  mergefolders('/content/Train/pcrsalmo', '/content/Train/salmo')\n",
        "  mergefolders('/content/Train/pcrncd', '/content/Train/ncd')\n",
        "  mergefolders('/content/Train/pcrhealthy', '/content/Train/healthy')\n",
        "\n",
        "  # remove folders not needed\n",
        "\n",
        "  !rm -rf '/content/Train/pcrsalmo'\n",
        "  !rm -rf '/content/Train/pcrncd'\n",
        "  !rm -rf '/content/Train/pcrhealthy'                    \n",
        "  !rm -rf '/content/Train/pcrcocci'\n",
        "\n",
        "  # Check folder counts\n",
        "  print(f\"cocci {len(os.listdir('/content/Train/cocci'))}\")\n",
        "  print(f\"healthy {len(os.listdir('/content/Train/healthy'))}\")\n",
        "  print(f\"ncd {len(os.listdir('/content/Train/ncd'))}\")\n",
        "  print(f\"salmonella {len(os.listdir('/content/Train/salmo'))}\")\n",
        "\n",
        "  # split data into test, train and validate partitions \n",
        "\n",
        "  splitfolders.ratio('Train', output=\"output\", seed=1337, ratio=(train,test,validate))\n"
      ],
      "metadata": {
        "id": "vdg0jY2mh7GX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_size(pixel_size = 224, batch_size = 128):\n",
        "  \"\"\"\n",
        "    Create variables with the ready split data\n",
        "    Allows the pizel size and batch size to be set\n",
        "    Autotunes the data to allow increased model speed\n",
        "    Returns three variables in the order train, test and validate\n",
        "\n",
        "    pixel_size: int - Single number (x) which is used to set the pixel size of the image to (x,x)\n",
        "    batch_size: int - Size of batch of images for training the model\n",
        "    \"\"\"\n",
        "# creates variable with pixel size and batch size altered \n",
        "  ds_train_ = image_dataset_from_directory(\n",
        "      '/content/output/train', # sets source of images\n",
        "      labels='inferred', # infers labels from folder names\n",
        "      label_mode='categorical',\n",
        "      image_size=[pixel_size, pixel_size], # allows pixel size to be altered by function call\n",
        "      interpolation='nearest',\n",
        "      batch_size=batch_size, # allows batch size to be altered by function call\n",
        "      shuffle=True,\n",
        "  )\n",
        "\n",
        "  ds_valid_ = image_dataset_from_directory(\n",
        "      '/content/output/val', # sets source of images\n",
        "      labels='inferred',\n",
        "      label_mode='categorical',\n",
        "      image_size=[pixel_size, pixel_size], # allows pixel size to be altered by function call\n",
        "      interpolation='nearest',\n",
        "      batch_size=batch_size, # allows batch size to be altered by function call\n",
        "      shuffle=False,\n",
        "  )\n",
        "\n",
        "  ds_test_ = image_dataset_from_directory(\n",
        "      '/content/output/test', # sets source of images\n",
        "      labels='inferred',\n",
        "      label_mode='categorical',\n",
        "      image_size=[pixel_size, pixel_size],# allows pixel size to be altered by function call\n",
        "      interpolation='nearest',\n",
        "      batch_size= batch_size, # allows batch size to be altered by function call\n",
        "      shuffle=False,\n",
        "  )\n",
        "  \n",
        "  # Autotunes the data to allow for faster model processing\n",
        "  AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "  ds_train_ = ds_train_.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "  ds_test_ = ds_test_.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "  ds_valid_ = ds_valid_.cache().prefetch(buffer_size=AUTOTUNE)\n",
        " \n",
        "  return [ds_train_, ds_test_, ds_valid_]"
      ],
      "metadata": {
        "id": "lVKuXjd9jMCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Need to add kaggle.json file to allow data download from kaggle\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "\n",
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "tS4AbMeviJtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a data augmentation layer to include in models\n",
        "data_augmentation = keras.Sequential([\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomFlip(\"vertical\"),\n",
        "        layers.RandomZoom(0.2),\n",
        "        layers.RandomRotation(factor = 0.2),\n",
        "        layers.RandomTranslation(height_factor=0.1, width_factor=0.1),\n",
        "    ])"
      ],
      "metadata": {
        "id": "JKvYZUg4_De0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_filepath = '/content/Models' # set location to save model\n",
        "callback = [ # create a list of callbacks to include in model\n",
        "    callbacks.EarlyStopping( # include early stopping to reduce overfitting\n",
        "    min_delta=0.001, # minimium amount of change to count as an improvement\n",
        "    patience=20, # how many epochs to wait before stopping\n",
        "    restore_best_weights=True,\n",
        "    monitor = 'val_accuracy',\n",
        "    verbose = 1\n",
        "),\n",
        "# include checkpoint to save best model as training progresses\n",
        "callbacks.ModelCheckpoint(filepath = checkpoint_filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max'),\n",
        "# include reduce learning rate on Plateau\n",
        "callbacks.ReduceLROnPlateau(monitor = 'val_accuracy', verbose = 1, factor = 0.5),\n",
        "]"
      ],
      "metadata": {
        "id": "9LS8d1ynOErs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inception = InceptionV3(include_top = False, input_shape = (224,224,3) )"
      ],
      "metadata": {
        "id": "lQmjTbKhLrhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# crate a function to create the final model\n",
        "def final_model(data = True, graph = True, epoch = 200, verbose = 1, fine_tune = 0 ):\n",
        "  \"\"\"\n",
        "  Function to train a compiled model incorporating a pretrained_base and a \n",
        "  head. It is compiled using the adam optimiser.\n",
        "\n",
        "  data: bool - If the history information should be printed, default is true\n",
        "  graph: bool - If the val_accuracy and val_loss graphs should be printed,\n",
        "  default is True\n",
        "  epoch: int - number of epochs for the model to run for\n",
        "  verbose: int - 1 for show progress, 0 for don't show progress\n",
        "  fine_tune: int - layers of the convolutional base to be trainable\n",
        "\n",
        "  \"\"\"\n",
        "  tf.get_logger().setLevel('ERROR') # reduce error messages shown\n",
        "\n",
        "  # Pretrained convolutional layers are loaded using the Imagenet weights.\n",
        "    # Include_top is set to False, in order to exclude the model's fully-connected layers.\n",
        "  conv_base = InceptionV3(include_top = False, input_shape = (224,224,3) )\n",
        "\n",
        "    # Defines how many layers to freeze during training.\n",
        "    # Layers in the convolutional base are switched from trainable to non-trainable\n",
        "    # depending on the size of the fine-tuning parameter.\n",
        "\n",
        "  if fine_tune > 0:\n",
        "        for layer in conv_base.layers[:-fine_tune]:\n",
        "            layer.trainable = False\n",
        "  else:\n",
        "        for layer in conv_base.layers:\n",
        "            layer.trainable = False\n",
        "\n",
        "  ## define the model including, augemntation, and trainable base\n",
        "  model = keras.Sequential([\n",
        "    tf.keras.layers.Rescaling(1./255),\n",
        "    data_augmentation,\n",
        "    inception,\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(4, activation='softmax'),\n",
        "  ])\n",
        "  # compile the model\n",
        "  model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy'],\n",
        "  )\n",
        "  #train the model\n",
        "  history = model.fit(\n",
        "    ds_train_,\n",
        "    validation_data=ds_valid_,\n",
        "    callbacks = [callback],\n",
        "    epochs=epoch,\n",
        "    verbose = verbose)\n",
        "  \n",
        "  # show results if required\n",
        "\n",
        "  history_frame = pd.DataFrame(history.history)\n",
        "  if data == True:\n",
        "    print(history_frame)\n",
        "  if graph == True:\n",
        "    history_frame.loc[1:, ['loss', 'val_loss']].plot()\n",
        "    history_frame.loc[1:, ['accuracy', 'val_accuracy']].plot();\n",
        "\n",
        "  return history\n",
        "\n"
      ],
      "metadata": {
        "id": "NfmcDbFuZfp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_data()\n",
        "ds_train_, ds_test_, ds_valid_ = data_size()"
      ],
      "metadata": {
        "id": "so57ESta0ecJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_model()"
      ],
      "metadata": {
        "id": "vvhF2hewJfYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Downloading the trained model"
      ],
      "metadata": {
        "id": "moGqFhdqK_vM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download model\n",
        "!wget -O model.zip https://www.dropbox.com/s/t49w8rr9jm5a36d/Models.zip?dl=0\n",
        "\n",
        "# unzip model\n",
        "!unzip /content/model.zip\n",
        "\n",
        "# Load model\n",
        "model = tf.keras.models.load_model('/content/Models')\n"
      ],
      "metadata": {
        "id": "wW46_EAPgcXW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}